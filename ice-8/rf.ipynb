{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "isuz-FH4bSGM",
        "outputId": "0af0c204-48fc-492a-e37b-9e8f7db26bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=a13aca676936528aa26bc4bd3baad454c1f1204ec896e6e821a288815fed7fc9\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "T1nOirvfcNMt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "load models\n",
        "'''\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer, IndexToString\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics"
      ],
      "metadata": {
        "id": "F1lUra6ObStc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to this Notebook instance\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6_BjnEvWbsFY",
        "outputId": "8abf583c-4112-4787-9d52-f469165c5035"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.read.format(\"libsvm\").load(\"/content/drive/My Drive/dataset.txt\")"
      ],
      "metadata": {
        "id": "7eX66NtMb2Sj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.select(\"features\").show(1,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XNK3KUNccvJP",
        "outputId": "8e42f781-3025-4d5b-daaf-e34e5914fe9d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------+\n",
            "|features                                         |\n",
            "+-------------------------------------------------+\n",
            "|(4,[0,1,2,3],[-0.222222,0.5,-0.762712,-0.833333])|\n",
            "+-------------------------------------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)"
      ],
      "metadata": {
        "id": "eGPgqUQAcvnj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "featureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)"
      ],
      "metadata": {
        "id": "bUj-juOec1qi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
      ],
      "metadata": {
        "id": "9KrvfLiyc4qy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "for n in (7,5):\n",
        "  rf = RandomForestClassifier(numTrees=n,featuresCol=\"indexedFeatures\",labelCol=\"indexedLabel\")\n",
        "  rf_pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])\n",
        "  rf_model = rf_pipeline.fit(trainingData)\n",
        "  rf_predictions = rf_model.transform(testData)\n",
        "  print(rf_model.stages[2])\n",
        "  rf_predictions.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gL0jIs_8c9Wp",
        "outputId": "f61600ec-58ab-4325-998e-c1d57be54ffd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassificationModel: uid=RandomForestClassifier_6334c0ec4696, numTrees=7, numClasses=3, numFeatures=4\n",
            "+-----+--------------------+------------+--------------------+--------------------+--------------------+----------+\n",
            "|label|            features|indexedLabel|     indexedFeatures|       rawPrediction|         probability|prediction|\n",
            "+-----+--------------------+------------+--------------------+--------------------+--------------------+----------+\n",
            "|  0.0|(4,[0,1,2,3],[-0....|         0.0|(4,[0,1,2,3],[-0....|[5.83333333333333...|[0.83333333333333...|       0.0|\n",
            "|  0.0|(4,[0,1,2,3],[-0....|         0.0|(4,[0,1,2,3],[-0....|[5.83333333333333...|[0.83333333333333...|       0.0|\n",
            "|  0.0|(4,[0,1,2,3],[-0....|         0.0|(4,[0,1,2,3],[-0....|       [7.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
            "|  0.0|(4,[0,1,2,3],[0.1...|         0.0|(4,[0,1,2,3],[0.1...|[6.83333333333333...|[0.97619047619047...|       0.0|\n",
            "|  0.0|(4,[0,1,2,3],[0.1...|         0.0|(4,[0,1,2,3],[0.1...|       [7.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
            "+-----+--------------------+------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "RandomForestClassificationModel: uid=RandomForestClassifier_f991bb141c24, numTrees=5, numClasses=3, numFeatures=4\n",
            "+-----+--------------------+------------+--------------------+--------------------+--------------------+----------+\n",
            "|label|            features|indexedLabel|     indexedFeatures|       rawPrediction|         probability|prediction|\n",
            "+-----+--------------------+------------+--------------------+--------------------+--------------------+----------+\n",
            "|  0.0|(4,[0,1,2,3],[-0....|         0.0|(4,[0,1,2,3],[-0....|       [5.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
            "|  0.0|(4,[0,1,2,3],[-0....|         0.0|(4,[0,1,2,3],[-0....|       [5.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
            "|  0.0|(4,[0,1,2,3],[-0....|         0.0|(4,[0,1,2,3],[-0....|       [5.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
            "|  0.0|(4,[0,1,2,3],[0.1...|         0.0|(4,[0,1,2,3],[0.1...|[4.94736842105263...|[0.98947368421052...|       0.0|\n",
            "|  0.0|(4,[0,1,2,3],[0.1...|         0.0|(4,[0,1,2,3],[0.1...|[4.94736842105263...|[0.98947368421052...|       0.0|\n",
            "+-----+--------------------+------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# calculate accuracy\n",
        "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = accuracy_evaluator.evaluate(rf_predictions)\n",
        "\n",
        "# calculate precision\n",
        "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "precision = precision_evaluator.evaluate(rf_predictions)\n",
        "\n",
        "# calculate recall\n",
        "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "recall = recall_evaluator.evaluate(rf_predictions)\n",
        "\n",
        "# calculate F1 score\n",
        "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = f1_evaluator.evaluate(rf_predictions)\n",
        "\n",
        "# print the evaluation metrics\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1 score: \", f1_score)\n",
        "print(\"F1 score: \", f1_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "b0HOuD-SdZyK",
        "outputId": "78588309-42cf-4716-ab62-17350fe58b74"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9629629629629629\n",
            "Precision:  0.9673202614379085\n",
            "Recall:  0.9629629629629629\n",
            "F1 score:  0.9628858024691358\n",
            "F1 score:  0.9628858024691358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a confusion matrix\n",
        "predictionsAndLabels = rf_predictions.select(\"prediction\", \"indexedLabel\").rdd\n",
        "metrics = MulticlassMetrics(predictionsAndLabels)\n",
        "confusion_matrix = metrics.confusionMatrix().toArray()\n",
        "\n",
        "# print the confusion matrix\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix)\n",
        "\n",
        "# calculate precision, recall, and F1-score\n",
        "tp = confusion_matrix[1, 1]\n",
        "fp = confusion_matrix[0, 1]\n",
        "tn = confusion_matrix[0, 0]\n",
        "fn = confusion_matrix[1, 0]\n",
        "\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
        "\n",
        "# print the evaluation metrics\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1 score: \", f1_score)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2G7MmH5sfimP",
        "outputId": "e71963be-4824-414c-eace-c96408934de1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix:\n",
            "[[15.  0.  0.]\n",
            " [ 0. 23.  0.]\n",
            " [ 2.  0. 14.]]\n",
            "Precision:  1.0\n",
            "Recall:  1.0\n",
            "F1 score:  1.0\n",
            "Accuracy:  1.0\n"
          ]
        }
      ]
    }
  ]
}